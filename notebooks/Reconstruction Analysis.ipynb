{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general imports \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#importing sklearn module\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "#utils.py contains all the plot function.\n",
    "from tulog.utils import plot, plot_ts, plot_rws, plot_error, unroll_ts\n",
    "import keras\n",
    "import sys\n",
    "import math\n",
    "sys.path.append('/Users/ajayarora/miniconda3/envs/Orion/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from orion.data import load_signal, load_anomalies\n",
    "from orion.evaluation.contextual import contextual_f1_score\n",
    "from orion.primitives.timeseries_anomalies import find_anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1222819200</td>\n",
       "      <td>-0.366359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1222840800</td>\n",
       "      <td>-0.394108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1222862400</td>\n",
       "      <td>0.403625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1222884000</td>\n",
       "      <td>-0.362759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1222905600</td>\n",
       "      <td>-0.370746</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    timestamp     value\n",
       "0  1222819200 -0.366359\n",
       "1  1222840800 -0.394108\n",
       "2  1222862400  0.403625\n",
       "3  1222884000 -0.362759\n",
       "4  1222905600 -0.370746"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "signal = 'S-1'\n",
    "# load_signal function load the given dataset\n",
    "df = load_signal(signal)\n",
    "# Since this dataset is already labelled, we will use load_anomalies functions to get all the known anomalies.\n",
    "known_anomalies = load_anomalies(signal)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        start\n",
      "0  1398168000\n"
     ]
    }
   ],
   "source": [
    "print(known_anomalies.iloc[:, 0:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_segments_aggregate(X, interval, time_column, method=['mean']):\n",
    "    \"\"\"Aggregate values over given time span.\n",
    "    Args:\n",
    "        X (ndarray or pandas.DataFrame):\n",
    "            N-dimensional sequence of values.\n",
    "        interval (int):\n",
    "            Integer denoting time span to compute aggregation of.\n",
    "        time_column (int):\n",
    "            Column of X that contains time values.\n",
    "        method (str or list):\n",
    "            Optional. String describing aggregation method or list of strings describing multiple\n",
    "            aggregation methods. If not given, `mean` is used.\n",
    "    Returns:\n",
    "        ndarray, ndarray:\n",
    "            * Sequence of aggregated values, one column for each aggregation method.\n",
    "            * Sequence of index values (first index of each aggregated segment).\n",
    "    \"\"\"\n",
    "    #checking for the input datatype as numpy array and converting it to dataframe\n",
    "    if isinstance(X, np.ndarray):\n",
    "        X = pd.DataFrame(X)\n",
    "    #sorting the values on timestamp column and setting it as a index\n",
    "    X = X.sort_values(time_column).set_index(time_column)\n",
    "\n",
    "    if isinstance(method, str):\n",
    "        method = [method]\n",
    "\n",
    "    start_ts = X.index.values[0]\n",
    "    max_ts = X.index.values[-1]\n",
    "\n",
    "    values = list()\n",
    "    index = list()\n",
    "    while start_ts <= max_ts:\n",
    "        end_ts = start_ts + interval\n",
    "        subset = X.loc[start_ts:end_ts - 1]\n",
    "        aggregated = [\n",
    "            getattr(subset, agg)(skipna=True).values\n",
    "            for agg in method\n",
    "        ]\n",
    "        values.append(np.concatenate(aggregated))\n",
    "        index.append(start_ts)\n",
    "        start_ts = end_ts\n",
    "\n",
    "    return np.asarray(values), np.asarray(index)\n",
    "\n",
    "#here df is the given dataframe and \"timestamp\" is the required column to be altered.\n",
    "X, index = time_segments_aggregate(df, interval=1800, time_column='timestamp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using the simple scikit imputer\n",
    "imp = SimpleImputer()\n",
    "X = imp.fit_transform(X)\n",
    "\n",
    "#Normalizing the data using scikit-learn MinMaxScalar\n",
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "X = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rolling_window_sequences(X, index, window_size, target_size, step_size, target_column,\n",
    "                             drop=None, drop_windows=False):\n",
    "    \"\"\"Create rolling window sequences out of time series data.\n",
    "    The function creates an array of input sequences and an array of target sequences by rolling\n",
    "    over the input sequence with a specified window.\n",
    "    Optionally, certain values can be dropped from the sequences.\n",
    "    Args:\n",
    "        X (ndarray):\n",
    "            N-dimensional sequence to iterate over.\n",
    "        index (ndarray):\n",
    "            Array containing the index values of X.\n",
    "        window_size (int):\n",
    "            Length of the input sequences.\n",
    "        target_size (int):\n",
    "            Length of the target sequences.\n",
    "        step_size (int):\n",
    "            Indicating the number of steps to move the window forward each round.\n",
    "        target_column (int):\n",
    "            Indicating which column of X is the target.\n",
    "        drop (ndarray or None or str or float or bool):\n",
    "            Optional. Array of boolean values indicating which values of X are invalid, or value\n",
    "            indicating which value should be dropped. If not given, `None` is used.\n",
    "        drop_windows (bool):\n",
    "            Optional. Indicates whether the dropping functionality should be enabled. If not\n",
    "            given, `False` is used.\n",
    "    Returns:\n",
    "        ndarray, ndarray, ndarray, ndarray:\n",
    "            * input sequences.\n",
    "            * target sequences.\n",
    "            * first index value of each input sequence.\n",
    "            * first index value of each target sequence.\n",
    "    \"\"\"\n",
    "    out_X = list()\n",
    "    out_y = list()\n",
    "    X_index = list()\n",
    "    y_index = list()\n",
    "    target = X[:, target_column]\n",
    "\n",
    "    if drop_windows:\n",
    "        if hasattr(drop, '__len__') and (not isinstance(drop, str)):\n",
    "            if len(drop) != len(X):\n",
    "                raise Exception('Arrays `drop` and `X` must be of the same length.')\n",
    "        else:\n",
    "            if isinstance(drop, float) and np.isnan(drop):\n",
    "                drop = np.isnan(X)\n",
    "            else:\n",
    "                drop = X == drop\n",
    "\n",
    "    start = 0\n",
    "    max_start = len(X) - window_size - target_size + 1\n",
    "    while start < max_start:\n",
    "        end = start + window_size\n",
    "\n",
    "        if drop_windows:\n",
    "            drop_window = drop[start:end + target_size]\n",
    "            to_drop = np.where(drop_window)[0]\n",
    "            if to_drop.size:\n",
    "                start += to_drop[-1] + 1\n",
    "                continue\n",
    "\n",
    "        out_X.append(X[start:end])\n",
    "        out_y.append(target[end:end + target_size])\n",
    "        X_index.append(index[start])\n",
    "        y_index.append(index[end])\n",
    "        start = start + step_size\n",
    "\n",
    "    return np.asarray(out_X), np.asarray(out_y), np.asarray(X_index), np.asarray(y_index)\n",
    "#the target value; the value at time t.\n",
    "#previous observed values, this is determined by the window width.\n",
    "X, y, X_index, y_index = rolling_window_sequences(X, index, \n",
    "                                                  window_size=100, \n",
    "                                                  target_size=1, \n",
    "                                                  step_size=1,\n",
    "                                                  target_column=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "from keras.layers import Input\n",
    "from keras.layers.merge import _Merge\n",
    "from keras.models import Model\n",
    "from mlprimitives.adapters.keras import build_layer\n",
    "from mlprimitives.utils import import_object\n",
    "from scipy import stats\n",
    "\n",
    "class RandomWeightedAverage(_Merge):\n",
    "    def _merge_function(self, inputs):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            inputs[0] x     original input\n",
    "            inputs[1] x_    predicted input\n",
    "        \"\"\"\n",
    "        alpha = K.random_uniform((64, 1, 1))\n",
    "        return (alpha * inputs[0]) + ((1 - alpha) * inputs[1])\n",
    "\n",
    "class TadGAN(object):\n",
    "    \"\"\"TadGAN model for time series reconstruction.\n",
    "\n",
    "    Args:\n",
    "        shape (tuple):\n",
    "            Tuple denoting the shape of an input sample.\n",
    "        encoder_input_shape (tuple):\n",
    "            Shape of encoder input.\n",
    "        generator_input_shape (tuple):\n",
    "            Shape of generator input.\n",
    "        critic_x_input_shape (tuple):\n",
    "            Shape of critic_x input.\n",
    "        critic_z_input_shape (tuple):\n",
    "            Shape of critic_z input.\n",
    "        layers_encoder (list):\n",
    "            List containing layers of encoder.\n",
    "        layers_generator (list):\n",
    "            List containing layers of generator.\n",
    "        layers_critic_x (list):\n",
    "            List containing layers of critic_x.\n",
    "        layers_critic_z (list):\n",
    "            List containing layers of critic_z.\n",
    "        optimizer (str):\n",
    "            String denoting the keras optimizer.\n",
    "        learning_rate (float):\n",
    "            Optional. Float denoting the learning rate of the optimizer. Default 0.005.\n",
    "        epochs (int):\n",
    "            Optional. Integer denoting the number of epochs. Default 2000.\n",
    "        latent_dim (int):\n",
    "            Optional. Integer denoting dimension of latent space. Default 20.\n",
    "        batch_size (int):\n",
    "            Integer denoting the batch size. Default 64.\n",
    "        iterations_critic (int):\n",
    "            Optional. Integer denoting the number of critic training steps per one\n",
    "            Generator/Encoder training step. Default 5.\n",
    "        hyperparameters (dictionary):\n",
    "            Optional. Dictionary containing any additional inputs.\n",
    "    \"\"\"\n",
    "\n",
    "    def __getstate__(self):\n",
    "        networks = ['critic_x', 'critic_z', 'encoder', 'generator']\n",
    "        modules = ['optimizer', 'critic_x_model', 'critic_z_model', 'encoder_generator_model']\n",
    "\n",
    "        state = self.__dict__.copy()\n",
    "\n",
    "        for module in modules:\n",
    "            del state[module]\n",
    "\n",
    "        for network in networks:\n",
    "            with tempfile.NamedTemporaryFile(suffix='.hdf5', delete=False) as fd:\n",
    "                keras.models.save_model(state.pop(network), fd.name, overwrite=True)\n",
    "\n",
    "                state[network + '_str'] = fd.read()\n",
    "\n",
    "        return state\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        networks = ['critic_x', 'critic_z', 'encoder', 'generator']\n",
    "        for network in networks:\n",
    "            with tempfile.NamedTemporaryFile(suffix='.hdf5', delete=False) as fd:\n",
    "                fd.write(state.pop(network + '_str'))\n",
    "                fd.flush()\n",
    "\n",
    "                state[network] = keras.models.load_model(fd.name)\n",
    "\n",
    "        self.__dict__ = state\n",
    "\n",
    "    def _build_model(self, hyperparameters, layers, input_shape):\n",
    "        x = Input(shape=input_shape)\n",
    "        model = keras.models.Sequential()\n",
    "\n",
    "        for layer in layers:\n",
    "            built_layer = build_layer(layer, hyperparameters)\n",
    "            model.add(built_layer)\n",
    "\n",
    "        return Model(x, model(x))\n",
    "\n",
    "    def _wasserstein_loss(self, y_true, y_pred):\n",
    "        return K.mean(y_true * y_pred)\n",
    "\n",
    "    def _gradient_penalty_loss(self, y_true, y_pred, averaged_samples):\n",
    "        gradients = K.gradients(y_pred, averaged_samples)[0]\n",
    "        gradients_sqr = K.square(gradients)\n",
    "        gradients_sqr_sum = K.sum(gradients_sqr, axis=np.arange(1, len(gradients_sqr.shape)))\n",
    "        gradient_l2_norm = K.sqrt(gradients_sqr_sum)\n",
    "        gradient_penalty = K.square(1 - gradient_l2_norm)\n",
    "        return K.mean(gradient_penalty)\n",
    "\n",
    "    def __init__(self, shape, encoder_input_shape, generator_input_shape, critic_x_input_shape,\n",
    "                 critic_z_input_shape, layers_encoder, layers_generator, layers_critic_x,\n",
    "                 layers_critic_z, optimizer, learning_rate=0.0005, epochs=2000, latent_dim=20,\n",
    "                 batch_size=64, iterations_critic=5, **hyperparameters):\n",
    "\n",
    "        self.shape = shape\n",
    "        self.latent_dim = latent_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.iterations_critic = iterations_critic\n",
    "        self.epochs = epochs\n",
    "        self.hyperparameters = hyperparameters\n",
    "\n",
    "        self.encoder_input_shape = encoder_input_shape\n",
    "        self.generator_input_shape = generator_input_shape\n",
    "        self.critic_x_input_shape = critic_x_input_shape\n",
    "        self.critic_z_input_shape = critic_z_input_shape\n",
    "\n",
    "        self.layers_encoder, self.layers_generator = layers_encoder, layers_generator\n",
    "        self.layers_critic_x, self.layers_critic_z = layers_critic_x, layers_critic_z\n",
    "\n",
    "        self.optimizer = import_object(optimizer)(learning_rate)\n",
    "\n",
    "    def _build_tadgan(self, **kwargs):\n",
    "\n",
    "        hyperparameters = self.hyperparameters.copy()\n",
    "        hyperparameters.update(kwargs)\n",
    "\n",
    "        self.encoder = self._build_model(hyperparameters, self.layers_encoder,\n",
    "                                         self.encoder_input_shape)\n",
    "        self.generator = self._build_model(hyperparameters, self.layers_generator,\n",
    "                                           self.generator_input_shape)\n",
    "        self.critic_x = self._build_model(hyperparameters, self.layers_critic_x,\n",
    "                                          self.critic_x_input_shape)\n",
    "        self.critic_z = self._build_model(hyperparameters, self.layers_critic_z,\n",
    "                                          self.critic_z_input_shape)\n",
    "\n",
    "        self.generator.trainable = False\n",
    "        self.encoder.trainable = False\n",
    "\n",
    "        z = Input(shape=(self.latent_dim, 1))\n",
    "        x = Input(shape=self.shape)\n",
    "        x_ = self.generator(z)\n",
    "        z_ = self.encoder(x)\n",
    "        fake_x = self.critic_x(x_)\n",
    "        valid_x = self.critic_x(x)\n",
    "        interpolated_x = RandomWeightedAverage()([x, x_])\n",
    "        validity_interpolated_x = self.critic_x(interpolated_x)\n",
    "        partial_gp_loss_x = partial(self._gradient_penalty_loss, averaged_samples=interpolated_x)\n",
    "        partial_gp_loss_x.__name__ = 'gradient_penalty'\n",
    "        self.critic_x_model = Model(inputs=[x, z], outputs=[valid_x, fake_x,\n",
    "                                                            validity_interpolated_x])\n",
    "        self.critic_x.trainable = True\n",
    "        self.critic_x_model.compile(loss=[self._wasserstein_loss, self._wasserstein_loss,\n",
    "                                          partial_gp_loss_x], optimizer=self.optimizer,\n",
    "                                    loss_weights=[1, 1, 10])\n",
    "\n",
    "        fake_z = self.critic_z(z_)\n",
    "        valid_z = self.critic_z(z)\n",
    "        interpolated_z = RandomWeightedAverage()([z, z_])\n",
    "        validity_interpolated_z = self.critic_z(interpolated_z)\n",
    "        partial_gp_loss_z = partial(self._gradient_penalty_loss, averaged_samples=interpolated_z)\n",
    "        partial_gp_loss_z.__name__ = 'gradient_penalty'\n",
    "        self.critic_z_model = Model(inputs=[x, z], outputs=[valid_z, fake_z,\n",
    "                                                            validity_interpolated_z])\n",
    "        self.critic_z.trainable = True\n",
    "        self.critic_z_model.compile(loss=[self._wasserstein_loss, self._wasserstein_loss,\n",
    "                                          partial_gp_loss_z], optimizer=self.optimizer,\n",
    "                                    loss_weights=[1, 1, 10])\n",
    "\n",
    "        \n",
    "        self.critic_x.trainable = False\n",
    "        self.critic_z.trainable = False\n",
    "        self.generator.trainable = True\n",
    "        self.encoder.trainable = True\n",
    "\n",
    "        z_gen = Input(shape=(self.latent_dim, 1))\n",
    "        x_gen_ = self.generator(z_gen)\n",
    "        x_gen = Input(shape=self.shape)\n",
    "        z_gen_ = self.encoder(x_gen)\n",
    "        x_gen_rec = self.generator(z_gen_)\n",
    "        fake_gen_x = self.critic_x(x_gen_)\n",
    "        fake_gen_z = self.critic_z(z_gen_)\n",
    "\n",
    "        self.encoder_generator_model = Model([x_gen, z_gen], [fake_gen_x, fake_gen_z, x_gen_rec])\n",
    "        self.encoder_generator_model.compile(loss=[self._wasserstein_loss, self._wasserstein_loss,\n",
    "                                                   'mse'], optimizer=self.optimizer,\n",
    "                                             loss_weights=[1, 1, 10])\n",
    "\n",
    "    def _fit(self, X):\n",
    "        fake = np.ones((self.batch_size, 1))\n",
    "        valid = -np.ones((self.batch_size, 1))\n",
    "        delta = np.ones((self.batch_size, 1))\n",
    "\n",
    "        X_ = np.copy(X)\n",
    "        for epoch in range(1, self.epochs + 1):\n",
    "            np.random.shuffle(X_)\n",
    "            epoch_g_loss = []\n",
    "            epoch_cx_loss = []\n",
    "            epoch_cz_loss = []\n",
    "\n",
    "            minibatches_size = self.batch_size * self.iterations_critic\n",
    "            num_minibatches = int(X_.shape[0] // minibatches_size)\n",
    "\n",
    "            for i in range(num_minibatches):\n",
    "                minibatch = X_[i * minibatches_size: (i + 1) * minibatches_size]\n",
    "\n",
    "                for j in range(self.iterations_critic):\n",
    "                    x = minibatch[j * self.batch_size: (j + 1) * self.batch_size]\n",
    "                    z = np.random.normal(size=(self.batch_size, self.latent_dim, 1))\n",
    "                    epoch_cx_loss.append(\n",
    "                        self.critic_x_model.train_on_batch([x, z], [valid, fake, delta]))\n",
    "                    epoch_cz_loss.append(\n",
    "                        self.critic_z_model.train_on_batch([x, z], [valid, fake, delta]))\n",
    "\n",
    "                epoch_g_loss.append(\n",
    "                    self.encoder_generator_model.train_on_batch([x, z], [valid, valid, x]))\n",
    "\n",
    "            cx_loss = np.mean(np.array(epoch_cx_loss), axis=0)\n",
    "            cz_loss = np.mean(np.array(epoch_cz_loss), axis=0)\n",
    "            g_loss = np.mean(np.array(epoch_g_loss), axis=0)\n",
    "            print('Epoch: {}/{}, [Dx loss: {}] [Dz loss: {}] [G loss: {}]'.format(\n",
    "                epoch, self.epochs, cx_loss, cz_loss, g_loss))\n",
    "            \n",
    "    def _fit_custom(self, X):\n",
    "        mse_errors = []\n",
    "        f1_scores = []\n",
    "        \n",
    "        fake = np.ones((self.batch_size, 1))\n",
    "        valid = -np.ones((self.batch_size, 1))\n",
    "        delta = np.ones((self.batch_size, 1))\n",
    "\n",
    "        X_ = np.copy(X)\n",
    "        for epoch in range(1, self.epochs + 1):\n",
    "            np.random.shuffle(X_)\n",
    "            epoch_g_loss = []\n",
    "            epoch_cx_loss = []\n",
    "            epoch_cz_loss = []\n",
    "\n",
    "            minibatches_size = self.batch_size * self.iterations_critic\n",
    "            num_minibatches = int(X_.shape[0] // minibatches_size)\n",
    "\n",
    "            for i in range(num_minibatches):\n",
    "                minibatch = X_[i * minibatches_size: (i + 1) * minibatches_size]\n",
    "\n",
    "                for j in range(self.iterations_critic):\n",
    "                    x = minibatch[j * self.batch_size: (j + 1) * self.batch_size]\n",
    "                    z = np.random.normal(size=(self.batch_size, self.latent_dim, 1))\n",
    "                    epoch_cx_loss.append(\n",
    "                        self.critic_x_model.train_on_batch([x, z], [valid, fake, delta]))\n",
    "                    epoch_cz_loss.append(\n",
    "                        self.critic_z_model.train_on_batch([x, z], [valid, fake, delta]))\n",
    "\n",
    "                epoch_g_loss.append(\n",
    "                    self.encoder_generator_model.train_on_batch([x, z], [valid, valid, x]))\n",
    "\n",
    "            cx_loss = np.mean(np.array(epoch_cx_loss), axis=0)\n",
    "            cz_loss = np.mean(np.array(epoch_cz_loss), axis=0)\n",
    "            g_loss = np.mean(np.array(epoch_g_loss), axis=0)\n",
    "            print('Epoch: {}/{}, [Dx loss: {}] [Dz loss: {}] [G loss: {}]'.format(\n",
    "                epoch, self.epochs, cx_loss, cz_loss, g_loss))\n",
    "            \n",
    "            if epoch % 2 == 0:\n",
    "                X_hat, critic = self.predict(X)\n",
    "                \n",
    "                # calculate MSE between y and y_hat\n",
    "                y_hat = unroll_ts(X_hat)\n",
    "                loss = mean_squared_error(y, y_hat)\n",
    "                mse_errors.append(loss)\n",
    "\n",
    "                # visualize the error curve\n",
    "                error, true_index, true, pred = score_anomalies(X, X_hat, critic, X_index, rec_error_type=\"dtw\", comb=\"mult\")\n",
    "                pred = np.array(pred).mean(axis=2)\n",
    "                plot_error([[true, pred], error])\n",
    "                \n",
    "                # calculate F1 score for anomalies\n",
    "                intervals = find_anomalies(error, index, \n",
    "                                           window_size_portion=0.33, \n",
    "                                           window_step_size_portion=0.1, \n",
    "                                           fixed_threshold=True)\n",
    "\n",
    "                ground_truth = known_anomalies.values.tolist()\n",
    "                anomalies = intervals.iloc[:, 0:2].values.tolist()\n",
    "                \n",
    "                # accuracy = contextual_accuracy(ground_truth, anomalies, start=start, end=end)\n",
    "                # accuracies.append(accuracy)\n",
    "                f1_score = contextual_f1_score(ground_truth, anomalies, start=start, end=end)\n",
    "                f1_scores.append(f1_score)\n",
    "                \n",
    "        return mse_errors, f1_scores, anomalies\n",
    "\n",
    "                \n",
    "\n",
    "    def fit(self, X, **kwargs):\n",
    "        \"\"\"Fit the TadGAN.\n",
    "\n",
    "        Args:\n",
    "            X (ndarray):\n",
    "                N-dimensional array containing the input training sequences for the model.\n",
    "        \"\"\"\n",
    "        self._build_tadgan(**kwargs)\n",
    "        X = X.reshape((-1, self.shape[0], 1))\n",
    "        mse, f1 = self._fit_custom(X)\n",
    "        return mse, f1\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict values using the initialized object.\n",
    "\n",
    "        Args:\n",
    "            X (ndarray):\n",
    "                N-dimensional array containing the input sequences for the model.\n",
    "\n",
    "        Returns:\n",
    "            ndarray:\n",
    "                N-dimensional array containing the reconstructions for each input sequence.\n",
    "            ndarray:\n",
    "                N-dimensional array containing the critic scores for each input sequence.\n",
    "        \"\"\"\n",
    "\n",
    "        X = X.reshape((-1, self.shape[0], 1))\n",
    "        z_ = self.encoder.predict(X)\n",
    "        y_hat = self.generator.predict(z_)\n",
    "        critic = self.critic_x.predict(X)\n",
    "\n",
    "        return y_hat, critic\n",
    "\n",
    "\n",
    "def _compute_critic_score(critics, smooth_window):\n",
    "    \"\"\"Compute an array of anomaly scores.\n",
    "\n",
    "    Args:\n",
    "        critics (ndarray):\n",
    "            Critic values.\n",
    "        smooth_window (int):\n",
    "            Smooth window that will be applied to compute smooth errors.\n",
    "\n",
    "    Returns:\n",
    "        ndarray:\n",
    "            Array of anomaly scores.\n",
    "    \"\"\"\n",
    "    critics = np.asarray(critics)\n",
    "    l_quantile = np.quantile(critics, 0.25)\n",
    "    u_quantile = np.quantile(critics, 0.75)\n",
    "    in_range = np.logical_and(critics >= l_quantile, critics <= u_quantile)\n",
    "    critic_mean = np.mean(critics[in_range])\n",
    "    critic_std = np.std(critics)\n",
    "\n",
    "    z_scores = np.absolute((np.asarray(critics) - critic_mean) / critic_std) + 1\n",
    "    z_scores = pd.Series(z_scores).rolling(\n",
    "        smooth_window, center=True, min_periods=smooth_window // 2).mean().values\n",
    "\n",
    "    return z_scores\n",
    "\n",
    "\n",
    "def score_anomalies(y, y_hat, critic, index, score_window=10, critic_smooth_window=None,\n",
    "                    error_smooth_window=None, smooth=True, rec_error_type=\"point\", comb=\"mult\",\n",
    "                    lambda_rec=0.5):\n",
    "    \"\"\"Compute an array of anomaly scores.\n",
    "\n",
    "    Anomaly scores are calculated using a combination of reconstruction error and critic score.\n",
    "\n",
    "    Args:\n",
    "        y (ndarray):\n",
    "            Ground truth.\n",
    "        y_hat (ndarray):\n",
    "            Predicted values. Each timestamp has multiple predictions.\n",
    "        index (ndarray):\n",
    "            time index for each y (start position of the window)\n",
    "        critic (ndarray):\n",
    "            Critic score. Each timestamp has multiple critic scores.\n",
    "        score_window (int):\n",
    "            Optional. Size of the window over which the scores are calculated.\n",
    "            If not given, 10 is used.\n",
    "        critic_smooth_window (int):\n",
    "            Optional. Size of window over which smoothing is applied to critic.\n",
    "            If not given, 200 is used.\n",
    "        error_smooth_window (int):\n",
    "            Optional. Size of window over which smoothing is applied to error.\n",
    "            If not given, 200 is used.\n",
    "        smooth (bool):\n",
    "            Optional. Indicates whether errors should be smoothed.\n",
    "            If not given, `True` is used.\n",
    "        rec_error_type (str):\n",
    "            Optional. The method to compute reconstruction error. Can be one of\n",
    "            `[\"point\", \"area\", \"dtw\"]`. If not given, 'point' is used.\n",
    "        comb (str):\n",
    "            Optional. How to combine critic and reconstruction error. Can be one\n",
    "            of `[\"mult\", \"sum\", \"rec\"]`. If not given, 'mult' is used.\n",
    "        lambda_rec (float):\n",
    "            Optional. Used if `comb=\"sum\"` as a lambda weighted sum to combine\n",
    "            scores. If not given, 0.5 is used.\n",
    "\n",
    "    Returns:\n",
    "        ndarray:\n",
    "            Array of anomaly scores.\n",
    "    \"\"\"\n",
    "\n",
    "    critic_smooth_window = critic_smooth_window or math.trunc(y.shape[0] * 0.01)\n",
    "    error_smooth_window = error_smooth_window or math.trunc(y.shape[0] * 0.01)\n",
    "\n",
    "    step_size = 1  # expected to be 1\n",
    "\n",
    "    true_index = index  # no offset\n",
    "\n",
    "    true = [item[0] for item in y.reshape((y.shape[0], -1))]\n",
    "\n",
    "    for item in y[-1][1:]:\n",
    "        true.extend(item)\n",
    "\n",
    "    critic_extended = list()\n",
    "    for c in critic:\n",
    "        critic_extended.extend(np.repeat(c, y_hat.shape[1]).tolist())\n",
    "\n",
    "    critic_extended = np.asarray(critic_extended).reshape((-1, y_hat.shape[1]))\n",
    "\n",
    "    critic_kde_max = []\n",
    "    pred_length = y_hat.shape[1]\n",
    "    num_errors = y_hat.shape[1] + step_size * (y_hat.shape[0] - 1)\n",
    "\n",
    "    for i in range(num_errors):\n",
    "        critic_intermediate = []\n",
    "\n",
    "        for j in range(max(0, i - num_errors + pred_length), min(i + 1, pred_length)):\n",
    "            critic_intermediate.append(critic_extended[i - j, j])\n",
    "\n",
    "        if len(critic_intermediate) > 1:\n",
    "            discr_intermediate = np.asarray(critic_intermediate)\n",
    "            try:\n",
    "                critic_kde_max.append(discr_intermediate[np.argmax(\n",
    "                    stats.gaussian_kde(discr_intermediate)(critic_intermediate))])\n",
    "            except np.linalg.LinAlgError:\n",
    "                critic_kde_max.append(np.median(discr_intermediate))\n",
    "        else:\n",
    "            critic_kde_max.append(np.median(np.asarray(critic_intermediate)))\n",
    "\n",
    "    # Compute critic scores\n",
    "    critic_scores = _compute_critic_score(critic_kde_max, critic_smooth_window)\n",
    "\n",
    "    # Compute reconstruction scores\n",
    "    rec_scores, predictions = reconstruction_errors(\n",
    "        y, y_hat, step_size, score_window, error_smooth_window, smooth, rec_error_type)\n",
    "\n",
    "    rec_scores = stats.zscore(rec_scores)\n",
    "    rec_scores = np.clip(rec_scores, a_min=0, a_max=None) + 1\n",
    "\n",
    "    # Combine the two scores\n",
    "    if comb == \"mult\":\n",
    "        final_scores = np.multiply(critic_scores, rec_scores)\n",
    "\n",
    "    elif comb == \"sum\":\n",
    "        final_scores = (1 - lambda_rec) * (critic_scores - 1) + lambda_rec * (rec_scores - 1)\n",
    "\n",
    "    elif comb == \"rec\":\n",
    "        final_scores = rec_scores\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            'Unknown combination specified {}, use \"mult\", \"sum\", or \"rec\" instead.'.format(comb))\n",
    "\n",
    "    true = [[t] for t in true]\n",
    "    return final_scores, true_index, true, predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tulog.model import hyperparameters\n",
    "#from mlprimitives.utils import import_object\n",
    "\n",
    "hyperparameters[\"epochs\"] = 35\n",
    "hyperparameters[\"shape\"] = (100, 1) # based on the window size\n",
    "hyperparameters[\"optimizer\"] = \"keras.optimizers.Adam\"\n",
    "hyperparameters[\"learning_rate\"] = 0.0005\n",
    "hyperparameters[\"latent_dim\"] = 20\n",
    "hyperparameters[\"batch_size\"] = 64\n",
    "\n",
    "tgan = TadGAN(**hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ajayarora/miniconda3/envs/Orion/lib/python3.6/site-packages/keras/engine/training.py:297: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n",
      "/Users/ajayarora/miniconda3/envs/Orion/lib/python3.6/site-packages/keras/engine/training.py:297: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n",
      "/Users/ajayarora/miniconda3/envs/Orion/lib/python3.6/site-packages/keras/engine/training.py:297: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n",
      "/Users/ajayarora/miniconda3/envs/Orion/lib/python3.6/site-packages/keras/engine/training.py:297: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    }
   ],
   "source": [
    "# reconstruct\n",
    "mse, f1 = tgan.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error, true_index, true, pred = score_anomalies(X, X_hat, critic, X_index, rec_error_type=\"dtw\", comb=\"mult\")\n",
    "pred = np.array(pred).mean(axis=2)\n",
    "# visualize the error curve\n",
    "plot_error([[true, pred], error])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
